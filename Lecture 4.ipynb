{"cells":[{"cell_type":"code","source":["simpledata = [(1,\"Ehtisham\",\"Data\",\"PK\",100,),\n              (2,\"Ali\",\"Tech\",\"PR\",10,),\n              (3,\"Ahmed\",\"Software\",\"PAK\",15,)]\ncolumns = [\"ID\",\"Name\",\"Department\",\"City\",\"Marks\"]\ndf_1 = spark.createDataFrame(data=simpledata,schema=columns)\ndf_1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5cfa40b4-16fc-4404-aeee-c625e189f153","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+----------+----+-----+\n| ID|    Name|Department|City|Marks|\n+---+--------+----------+----+-----+\n|  1|Ehtisham|      Data|  PK|  100|\n|  2|     Ali|      Tech|  PR|   10|\n|  3|   Ahmed|  Software| PAK|   15|\n+---+--------+----------+----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["simpledata_1 = [(1,\"Muhammad\",\"Data\",\"PK\",80,),\n              (2,\"Yusuf\",\"Tech\",\"PR\",50,),\n              (3,\"Raheb\",\"Software\",\"PAK\",30,)]\ncolumns = [\"ID\",\"Name\",\"Department\",\"City\",\"Marks\"]\ndf_2 = spark.createDataFrame(data=simpledata_1,schema=columns)\ndf_2.show()       "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"333a6ff9-7143-43d5-bb17-3dd538903135","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+----------+----+-----+\n| ID|    Name|Department|City|Marks|\n+---+--------+----------+----+-----+\n|  1|Muhammad|      Data|  PK|   80|\n|  2|   Yusuf|      Tech|  PR|   50|\n|  3|   Raheb|  Software| PAK|   30|\n+---+--------+----------+----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import * \nwindow =Window.partitionBy().orderBy(col(\"Marks\").desc())\ndf_1.withColumn(\"r1\",row_number().over(window)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"66489f4c-d6b8-4a81-a84a-72110605cc87","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+----------+----+-----+---+\n| ID|    Name|Department|City|Marks| r1|\n+---+--------+----------+----+-----+---+\n|  1|Ehtisham|      Data|  PK|  100|  1|\n|  3|   Ahmed|  Software| PAK|   15|  2|\n|  2|     Ali|      Tech|  PR|   10|  3|\n+---+--------+----------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["simpledata = [(1,\"Ehtisham\",\"Data\",\"PK\",100,),\n              (2,\"Ali\",\"Tech\",\"PR\",10,),\n              (3,\"Ahmed\",\"Software\",\"PAK\",15,)]\ncolumns = [\"ID\",\"Name\",\"Department\",\"City\",\"Marks\"]\ndf_1 = spark.createDataFrame(data=simpledata,schema=columns)\ndf_1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e011b9b5-a70b-4a69-a61e-d13328c0068d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom random import randint\nfrom datetime import datetime, timedelta"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c22ad466-37f3-46b3-9f72-b121e4b5615f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"BorrowerLoans\").getOrCreate()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"884b7f90-f07b-43bf-9e7b-854ca45eed32","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["borrowers = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Hannah\", \"Ivy\", \"Jack\",\"Alice\",\"Bob\"]\nnum_rows = 12"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"62c337de-0897-497d-bf36-4c2d80cea1c0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = []\nfor i in range(num_rows):\n    borrower_id = i % len(borrowers)  # Cyclic assignment of borrower_id\n    amount_borrowed = randint(1000, 5000)\n    loan_id = f\"Loan-{i+1}\"\n    date = datetime(2023, 7, 1) - timedelta(days=i)  # Varying dates in descending order\n    data.append((borrowers[borrower_id], amount_borrowed, loan_id, borrower_id, date))\n\n# Define the schema and create the DataFrame\nschema = [\"borrower_name\", \"amount_borrowed\", \"loan_id\", \"borrower_id\", \"date\"]\ndf = spark.createDataFrame(data, schema=schema)\n\n# Show the DataFrame\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"12d398df-54cc-4036-945f-57d6a39bcc2e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+-------------------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|               date|\n+-------------+---------------+-------+-----------+-------------------+\n|        Alice|           3192| Loan-1|          0|2023-07-01 00:00:00|\n|          Bob|           4401| Loan-2|          1|2023-06-30 00:00:00|\n|      Charlie|           2095| Loan-3|          2|2023-06-29 00:00:00|\n|        David|           4443| Loan-4|          3|2023-06-28 00:00:00|\n|          Eva|           1358| Loan-5|          4|2023-06-27 00:00:00|\n|        Frank|           1176| Loan-6|          5|2023-06-26 00:00:00|\n|        Grace|           3220| Loan-7|          6|2023-06-25 00:00:00|\n|       Hannah|           4149| Loan-8|          7|2023-06-24 00:00:00|\n|          Ivy|           3454| Loan-9|          8|2023-06-23 00:00:00|\n|         Jack|           4962|Loan-10|          9|2023-06-22 00:00:00|\n|        Alice|           1326|Loan-11|         10|2023-06-21 00:00:00|\n|          Bob|           1500|Loan-12|         11|2023-06-20 00:00:00|\n+-------------+---------------+-------+-----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["simpledata_1 = [(\"Alice\",3192,1,1,\"2023-07-01\",),\n              (\"Bob\",2340,2,2,\"2023-06-30\",),\n              (\"Charlie\",2095,3,3,\"2023-06-29\",),\n              (\"Charlie\",2588,4,3,\"2023-06-20\",),\n              (\"Frank\",20950,5,4,\"2023-05-29\",),\n              (\"Hannah\",2478,6,5,\"2022-06-29\",),\n              (\"Jack\",7600,7,6,\"2021-05-25\",),\n              (\"Bob\",10000,8,7,\"2022-02-27\",),\n              (\"Grace\",6500,9,8,\"2020-06-29\",),\n              (\"Bob\",5500,10,2,\"2023-05-30\",),\n              (\"Alice\",1340,11,1,\"2020-06-30\",),\n              ]\ncolumns = [\"borrower_name\",\"amount_borrowed\",\"loan_id\",\"borrower_id\",\"date\"]\ndf_2 = spark.createDataFrame(data=simpledata_1,schema=columns)\ndf_2.show()       "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"729f8d52-bdc6-49e7-98e9-a601b7f7fe8d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|\n+-------------+---------------+-------+-----------+----------+\n|        Alice|           3192|      1|          1|2023-07-01|\n|          Bob|           2340|      2|          2|2023-06-30|\n|      Charlie|           2095|      3|          3|2023-06-29|\n|      Charlie|           2588|      4|          3|2023-06-20|\n|        Frank|          20950|      5|          4|2023-05-29|\n|       Hannah|           2478|      6|          5|2022-06-29|\n|         Jack|           7600|      7|          6|2021-05-25|\n|          Bob|          10000|      8|          7|2022-02-27|\n|        Grace|           6500|      9|          8|2020-06-29|\n|          Bob|           5500|     10|          2|2023-05-30|\n|        Alice|           1340|     11|          1|2020-06-30|\n+-------------+---------------+-------+-----------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = df_2.withColumn(\"date\", df_2[\"date\"].cast(\"date\"))\n# Define the Window specification\nwindow_spec = Window.partitionBy(\"borrower_id\").orderBy(\"date\")\n# Add a new column \"loan_rank\" to the DataFrame using the rank function\ndf = df.withColumn(\"loan_rank\", rank().over(window_spec))\n# Show the DataFrame with the loan ranks\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d0dce160-8d5d-49c7-9c94-780e61acd070","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+---------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|loan_rank|\n+-------------+---------------+-------+-----------+----------+---------+\n|        Alice|           1340|     11|          1|2020-06-30|        1|\n|        Alice|           3192|      1|          1|2023-07-01|        2|\n|          Bob|           5500|     10|          2|2023-05-30|        1|\n|          Bob|           2340|      2|          2|2023-06-30|        2|\n|      Charlie|           2588|      4|          3|2023-06-20|        1|\n|      Charlie|           2095|      3|          3|2023-06-29|        2|\n|        Frank|          20950|      5|          4|2023-05-29|        1|\n|       Hannah|           2478|      6|          5|2022-06-29|        1|\n|         Jack|           7600|      7|          6|2021-05-25|        1|\n|          Bob|          10000|      8|          7|2022-02-27|        1|\n|        Grace|           6500|      9|          8|2020-06-29|        1|\n+-------------+---------------+-------+-----------+----------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["filtered_df = df.filter(df.loan_rank == 1)\n# Show the filtered DataFrame\nfiltered_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"813d0ce9-f9da-4e99-a837-9279040b98fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+---------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|loan_rank|\n+-------------+---------------+-------+-----------+----------+---------+\n|        Alice|           1340|     11|          1|2020-06-30|        1|\n|          Bob|           5500|     10|          2|2023-05-30|        1|\n|      Charlie|           2588|      4|          3|2023-06-20|        1|\n|        Frank|          20950|      5|          4|2023-05-29|        1|\n|       Hannah|           2478|      6|          5|2022-06-29|        1|\n|         Jack|           7600|      7|          6|2021-05-25|        1|\n|          Bob|          10000|      8|          7|2022-02-27|        1|\n|        Grace|           6500|      9|          8|2020-06-29|        1|\n+-------------+---------------+-------+-----------+----------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import rank, dense_rank, row_number, sum, avg, max, min, lead, lag\n\nspark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate()\n\n# Sample data (replace this with your actual DataFrame)\ndata = [\n    (\"Alice\", 3192, 1, 1, \"2023-07-01\"),\n    (\"Bob\", 2340, 2, 2, \"2023-06-30\"),\n    (\"Charlie\", 2095, 3, 3, \"2023-06-29\"),\n    (\"Charlie\", 2588, 4, 3, \"2023-06-20\"),\n    (\"Frank\", 20950, 5, 4, \"2023-05-29\"),\n    (\"Hannah\", 2478, 6, 5, \"2022-06-29\"),\n    (\"Jack\", 7600, 7, 6, \"2021-05-25\"),\n    (\"Bob\", 10000, 8, 7, \"2022-02-27\"),\n    (\"Grace\", 6500, 9, 8, \"2020-06-29\"),\n    (\"Bob\", 5500, 10, 2, \"2023-05-30\"),\n    (\"Alice\", 1340, 11, 1, \"2020-06-30\"),\n]\n\nschema = [\"borrower_name\", \"amount_borrowed\", \"loan_id\", \"borrower_id\", \"date\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"841346f1-c0fa-4eb0-827f-eeb07dfb849e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndf = spark.createDataFrame(data, schema=schema)\ndf = df.withColumn(\"date\", df[\"date\"].cast(\"date\"))\nwindow_spec = Window.partitionBy(\"borrower_id\").orderBy(\"date\")\ndf = df.withColumn(\"total_amount_borrowed\", sum(\"amount_borrowed\").over(window_spec))\n# Show the DataFrame with the window functions applied\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"405a8d1b-31d6-4613-a5ae-23f59a319e46","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+---------------------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|total_amount_borrowed|\n+-------------+---------------+-------+-----------+----------+---------------------+\n|        Alice|           1340|     11|          1|2020-06-30|                 1340|\n|        Alice|           3192|      1|          1|2023-07-01|                 4532|\n|          Bob|           5500|     10|          2|2023-05-30|                 5500|\n|          Bob|           2340|      2|          2|2023-06-30|                 7840|\n|      Charlie|           2588|      4|          3|2023-06-20|                 2588|\n|      Charlie|           2095|      3|          3|2023-06-29|                 4683|\n|        Frank|          20950|      5|          4|2023-05-29|                20950|\n|       Hannah|           2478|      6|          5|2022-06-29|                 2478|\n|         Jack|           7600|      7|          6|2021-05-25|                 7600|\n|          Bob|          10000|      8|          7|2022-02-27|                10000|\n|        Grace|           6500|      9|          8|2020-06-29|                 6500|\n+-------------+---------------+-------+-----------+----------+---------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndf = spark.createDataFrame(data, schema=schema)\ndf = df.withColumn(\"date\", df[\"date\"].cast(\"date\"))\nwindow_spec = Window.partitionBy(\"borrower_id\").orderBy(\"date\")\ndf = df.withColumn(\"avg_amount_borrowed\", avg(\"amount_borrowed\").over(window_spec))\n# Show the DataFrame with the window functions applied\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cab76284-4d2a-4f6a-98a6-8e4336efcb6f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+-------------------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|avg_amount_borrowed|\n+-------------+---------------+-------+-----------+----------+-------------------+\n|        Alice|           1340|     11|          1|2020-06-30|             1340.0|\n|        Alice|           3192|      1|          1|2023-07-01|             2266.0|\n|          Bob|           5500|     10|          2|2023-05-30|             5500.0|\n|          Bob|           2340|      2|          2|2023-06-30|             3920.0|\n|      Charlie|           2588|      4|          3|2023-06-20|             2588.0|\n|      Charlie|           2095|      3|          3|2023-06-29|             2341.5|\n|        Frank|          20950|      5|          4|2023-05-29|            20950.0|\n|       Hannah|           2478|      6|          5|2022-06-29|             2478.0|\n|         Jack|           7600|      7|          6|2021-05-25|             7600.0|\n|          Bob|          10000|      8|          7|2022-02-27|            10000.0|\n|        Grace|           6500|      9|          8|2020-06-29|             6500.0|\n+-------------+---------------+-------+-----------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [\n    (\"Alice\", 3192, 1, 1, \"2023-07-01\"),\n    (\"Bob\", 2340, 2, 2, \"2023-06-30\"),\n    (\"Charlie\", 2095, 3, 3, \"2023-06-29\"),\n    (\"Charlie\", 2588, 4, 3, \"2023-06-20\"),\n    (\"Frank\", 20950, 5, 4, \"2023-05-29\"),\n    (\"Hannah\", 2478, 6, 5, \"2022-06-29\"),\n    (\"Jack\", 7600, 7, 6, \"2021-05-25\"),\n    (\"Bob\", 10000, 8, 7, \"2022-02-27\"),\n    (\"Grace\", 6500, 9, 8, \"2020-06-29\"),\n    (\"Bob\", 5500, 10, 2, \"2023-05-30\"),\n    (\"Alice\", 1340, 11, 1, \"2020-06-30\"),\n    (\"Alice\", 130, 12, 1, \"2019-06-30\"),\n    (\"Alice\", 1340, 13, 1, \"2024-06-30\"),\n]\n\nschema = [\"borrower_name\", \"amount_borrowed\", \"loan_id\", \"borrower_id\", \"date\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a258b521-3a7c-428d-801b-d9bc38b2fd0f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndf = spark.createDataFrame(data, schema=schema)\ndf = df.withColumn(\"date\", df[\"date\"].cast(\"date\"))\nwindow_spec = Window.partitionBy(\"borrower_id\").orderBy(\"date\")\ndf = df.withColumn(\"max_amount_borrowed\", max(\"amount_borrowed\").over(window_spec))\n# Show the DataFrame with the window functions applied\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dae6517e-4857-49b6-9b6f-f9a72e3dcef3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+-------------------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|max_amount_borrowed|\n+-------------+---------------+-------+-----------+----------+-------------------+\n|        Alice|            130|     12|          1|2019-06-30|                130|\n|        Alice|           1340|     11|          1|2020-06-30|               1340|\n|        Alice|           3192|      1|          1|2023-07-01|               3192|\n|        Alice|           1340|     13|          1|2024-06-30|               3192|\n|          Bob|           5500|     10|          2|2023-05-30|               5500|\n|          Bob|           2340|      2|          2|2023-06-30|               5500|\n|      Charlie|           2588|      4|          3|2023-06-20|               2588|\n|      Charlie|           2095|      3|          3|2023-06-29|               2588|\n|        Frank|          20950|      5|          4|2023-05-29|              20950|\n|       Hannah|           2478|      6|          5|2022-06-29|               2478|\n|         Jack|           7600|      7|          6|2021-05-25|               7600|\n|          Bob|          10000|      8|          7|2022-02-27|              10000|\n|        Grace|           6500|      9|          8|2020-06-29|               6500|\n+-------------+---------------+-------+-----------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndf = spark.createDataFrame(data, schema=schema)\ndf = df.withColumn(\"date\", df[\"date\"].cast(\"date\"))\nwindow_spec = Window.partitionBy(\"borrower_id\").orderBy(\"date\")\ndf = df.withColumn(\"min_amount_borrowed\", min(\"amount_borrowed\").over(window_spec))\n# Show the DataFrame with the window functions applied\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0808341c-42e5-452a-86cb-12804cbfe9fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+---------------+-------+-----------+----------+-------------------+\n|borrower_name|amount_borrowed|loan_id|borrower_id|      date|min_amount_borrowed|\n+-------------+---------------+-------+-----------+----------+-------------------+\n|        Alice|            130|     12|          1|2019-06-30|                130|\n|        Alice|           1340|     11|          1|2020-06-30|                130|\n|        Alice|           3192|      1|          1|2023-07-01|                130|\n|        Alice|           1340|     13|          1|2024-06-30|                130|\n|          Bob|           5500|     10|          2|2023-05-30|               5500|\n|          Bob|           2340|      2|          2|2023-06-30|               2340|\n|      Charlie|           2588|      4|          3|2023-06-20|               2588|\n|      Charlie|           2095|      3|          3|2023-06-29|               2095|\n|        Frank|          20950|      5|          4|2023-05-29|              20950|\n|       Hannah|           2478|      6|          5|2022-06-29|               2478|\n|         Jack|           7600|      7|          6|2021-05-25|               7600|\n|          Bob|          10000|      8|          7|2022-02-27|              10000|\n|        Grace|           6500|      9|          8|2020-06-29|               6500|\n+-------------+---------------+-------+-----------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d6f39e9-7bc6-4306-ab6b-1a61ca196f2f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Lecture 4","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
